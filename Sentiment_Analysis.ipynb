{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-wxk663NHKi",
        "outputId": "d18b5138-90f4-4663-e2eb-c15dcf5b5884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Review: I love this product! It works perfectly.\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.315, 'pos': 0.685, 'compound': 0.8655}\n",
            "Overall Sentiment: Positive\n",
            "--------------------------------------------------\n",
            "Review: This is the worst purchase I've ever made.\n",
            "Sentiment Scores: {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
            "Overall Sentiment: Negative\n",
            "--------------------------------------------------\n",
            "Review: The product is okay, not great but not terrible either.\n",
            "Sentiment Scores: {'neg': 0.154, 'neu': 0.503, 'pos': 0.343, 'compound': 0.3887}\n",
            "Overall Sentiment: Positive\n",
            "--------------------------------------------------\n",
            "Review: Excellent quality! Will definitely buy again.\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.374, 'pos': 0.626, 'compound': 0.7712}\n",
            "Overall Sentiment: Positive\n",
            "--------------------------------------------------\n",
            "Review: Terrible service. The product arrived broken.\n",
            "Sentiment Scores: {'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.7351}\n",
            "Overall Sentiment: Negative\n",
            "--------------------------------------------------\n",
            "Review: i am amey\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Overall Sentiment: Neutral\n",
            "--------------------------------------------------\n",
            "Review: i am a bad boy\n",
            "Sentiment Scores: {'neg': 0.636, 'neu': 0.364, 'pos': 0.0, 'compound': -0.5423}\n",
            "Overall Sentiment: Negative\n",
            "--------------------------------------------------\n",
            "Review: i am always happy\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.351, 'pos': 0.649, 'compound': 0.5719}\n",
            "Overall Sentiment: Positive\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install nltk\n",
        "\n",
        "# Import libraries\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define a list of reviews\n",
        "reviews = [\n",
        "    \"I love this product! It works perfectly.\",\n",
        "    \"This is the worst purchase I've ever made.\",\n",
        "    \"The product is okay, not great but not terrible either.\",\n",
        "    \"Excellent quality! Will definitely buy again.\",\n",
        "    \"Terrible service. The product arrived broken.\",\n",
        "    \"i am amey\",\n",
        "    \"i am a bad boy\",\n",
        "    \"i am always happy\"\n",
        "]\n",
        "\n",
        "# Function to analyze sentiment\n",
        "def analyze_sentiment(reviews):\n",
        "    for review in reviews:\n",
        "        scores = sia.polarity_scores(review)\n",
        "        print(f\"Review: {review}\")\n",
        "        print(f\"Sentiment Scores: {scores}\")\n",
        "        sentiment = \"Positive\" if scores['compound'] > 0 else \"Negative\" if scores['compound'] < 0 else \"Neutral\"\n",
        "        print(f\"Overall Sentiment: {sentiment}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Analyze the sentiment of the reviews\n",
        "analyze_sentiment(reviews)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ground truth labels\n",
        "ground_truth = [\"Positive\", \"Negative\", \"Neutral\", \"Positive\", \"Negative\", \"Neutral\", \"Negative\", \"Positive\"]\n",
        "\n",
        "# Define a function to compare predicted sentiments with ground truth and calculate accuracy\n",
        "def evaluate_sentiment_model(reviews, ground_truth):\n",
        "    correct_predictions = 0\n",
        "    total_reviews = len(reviews)\n",
        "    predictions = []\n",
        "\n",
        "    for review, true_label in zip(reviews, ground_truth):\n",
        "        scores = sia.polarity_scores(review)\n",
        "        sentiment = \"Positive\" if scores['compound'] > 0 else \"Negative\" if scores['compound'] < 0 else \"Neutral\"\n",
        "        predictions.append(sentiment)\n",
        "        if sentiment == true_label:\n",
        "            correct_predictions += 1\n",
        "        print(f\"Review: {review}\")\n",
        "        print(f\"Sentiment Scores: {scores}\")\n",
        "        print(f\"Predicted Sentiment: {sentiment}\")\n",
        "        print(f\"True Sentiment: {true_label}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    accuracy = correct_predictions / total_reviews\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return predictions\n",
        "\n",
        "# Evaluate the sentiment model\n",
        "predictions = evaluate_sentiment_model(reviews, ground_truth)\n"
      ],
      "metadata": {
        "id": "ysjmnp9ZPi-R",
        "outputId": "d205e982-4c4c-46bf-e093-6035b11b68cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: I love this product! It works perfectly.\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.315, 'pos': 0.685, 'compound': 0.8655}\n",
            "Predicted Sentiment: Positive\n",
            "True Sentiment: Positive\n",
            "--------------------------------------------------\n",
            "Review: This is the worst purchase I've ever made.\n",
            "Sentiment Scores: {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
            "Predicted Sentiment: Negative\n",
            "True Sentiment: Negative\n",
            "--------------------------------------------------\n",
            "Review: The product is okay, not great but not terrible either.\n",
            "Sentiment Scores: {'neg': 0.154, 'neu': 0.503, 'pos': 0.343, 'compound': 0.3887}\n",
            "Predicted Sentiment: Positive\n",
            "True Sentiment: Neutral\n",
            "--------------------------------------------------\n",
            "Review: Excellent quality! Will definitely buy again.\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.374, 'pos': 0.626, 'compound': 0.7712}\n",
            "Predicted Sentiment: Positive\n",
            "True Sentiment: Positive\n",
            "--------------------------------------------------\n",
            "Review: Terrible service. The product arrived broken.\n",
            "Sentiment Scores: {'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.7351}\n",
            "Predicted Sentiment: Negative\n",
            "True Sentiment: Negative\n",
            "--------------------------------------------------\n",
            "Review: i am amey\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Predicted Sentiment: Neutral\n",
            "True Sentiment: Neutral\n",
            "--------------------------------------------------\n",
            "Review: i am a bad boy\n",
            "Sentiment Scores: {'neg': 0.636, 'neu': 0.364, 'pos': 0.0, 'compound': -0.5423}\n",
            "Predicted Sentiment: Negative\n",
            "True Sentiment: Negative\n",
            "--------------------------------------------------\n",
            "Review: i am always happy\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.351, 'pos': 0.649, 'compound': 0.5719}\n",
            "Predicted Sentiment: Positive\n",
            "True Sentiment: Positive\n",
            "--------------------------------------------------\n",
            "Accuracy: 87.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(ground_truth, predictions, labels=[\"Positive\", \"Negative\", \"Neutral\"])\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(ground_truth, predictions, labels=[\"Positive\", \"Negative\", \"Neutral\"])\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "j-rEnjPuPet2",
        "outputId": "0a4d8d6f-36d8-4ec6-9250-c495f111a5a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[3 0 0]\n",
            " [0 3 0]\n",
            " [1 0 1]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.75      1.00      0.86         3\n",
            "    Negative       1.00      1.00      1.00         3\n",
            "     Neutral       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.88         8\n",
            "   macro avg       0.92      0.83      0.84         8\n",
            "weighted avg       0.91      0.88      0.86         8\n",
            "\n"
          ]
        }
      ]
    }
  ]
}